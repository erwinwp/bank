# -*- coding: utf-8 -*-
"""KELOMPOK_1_KODE_1_UAS_CONVLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQER11vLcUdWY4iRctqeN1fbniXvuj3v

# KELOMPOK I KODE 1 UAS

Disusun oleh:

Justin Ryan Pangestu 6162201005

Erwin Wongsoputra 6162201026

Nicholas Adi 6162201035

Paskalis Theo Cahyadi 6162201065

Dandel Rivaldo 6162201098

Jeremy Nathanael Koen 6162201111

Jason 6162201122

## CONVLSTM

## INSTALL DEPENDENCIES

Kode !pip install timm berfungsi untuk memasang library PyTorch Image Models (timm), yang menyediakan berbagai arsitektur model deep learning mutakhir khusus untuk pemrosesan citra, seperti Vision Transformer dan EfficientNet. Library ini memudahkan pengguna dalam mengimpor model-model siap pakai tanpa harus membangun arsitektur dari nol. Selanjutnya, !pip install torchviz digunakan untuk memasang library torchviz, yang berguna untuk memvisualisasikan computational graph dari model yang dibuat dengan PyTorch. Dengan visualisasi ini, pengguna dapat lebih mudah memahami alur data dalam model, melihat hubungan antar layer, serta membantu proses debugging. Terakhir, !apt-get install unrar digunakan untuk memasang tool unrar di sistem Linux. Tool ini diperlukan untuk mengekstrak file yang dikompresi dalam format .rar, yang sering digunakan untuk distribusi dataset atau file besar.
"""

!pip install timm
!pip install torchviz
!apt-get install unrar

"""## DOWNLOAD AND EXTRACT DATASET

Kode !wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar digunakan untuk mengunduh file dataset UCF50 dalam format .rar dari situs resminya. Opsi --no-check-certificate memungkinkan proses unduhan tetap berjalan meskipun ada masalah pada sertifikat SSL situs. Setelah file berhasil diunduh, perintah !unrar x UCF50.rar digunakan untuk mengekstrak isi file .rar tersebut, dengan tetap mempertahankan struktur folder aslinya. Ini dilakukan agar dataset siap digunakan dalam proses pelatihan atau analisis. Terakhir, !rm -fr UCF50.rar berfungsi untuk menghapus file .rar yang sudah diekstrak, guna membersihkan direktori kerja dan menghemat ruang penyimpanan. Secara keseluruhan, ketiga perintah ini membentuk proses standar untuk download → ekstrak → bersih-bersih ketika bekerja dengan dataset yang dikompresi.
"""

!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar
!unrar x UCF50.rar
!rm -fr UCF50.rar

"""## SET ENVIRONMENT VARIABLE

Kode berikut ini digunakan untuk mengoptimalkan penggunaan memori GPU oleh PyTorch. Melalui perintah %env PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128, environment variable PYTORCH_CUDA_ALLOC_CONF diatur agar proses garbage collection pada memori GPU lebih agresif (pada threshold 60%) dan agar alokasi blok memori GPU tidak melebihi 128 MB per permintaan. Tujuan utamanya adalah mengurangi kemungkinan error "CUDA out of memory" dan memperbaiki efisiensi penggunaan memori, terutama saat melatih model deep learning yang besar atau model dengan dynamic computational graph yang membuat alokasi memori GPU menjadi fluktuatif. Setelah environment variable diatur, runtime perlu di-restart secara manual supaya konfigurasi ini diterapkan dengan benar saat PyTorch di-import kembali. Setelah restart, pengguna dapat melanjutkan proses training atau inferensi dengan konfigurasi memori GPU yang lebih optimal. Trik ini biasanya digunakan ketika kita menghadapi model yang gagal berjalan karena kehabisan memori GPU, atau saat kita ingin meningkatkan stabilitas alokasi memori GPU saat melatih model kompleks. Untuk model kecil atau jika memori GPU cukup besar, trik ini tidak selalu diperlukan.
"""

# Commented out IPython magic to ensure Python compatibility.
# Set environment variable
# %env PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128

# Restart runtime secara manual (menu: Runtime > Restart Runtime)
# Lalu lanjutkan dengan import torch dan menjalankan kode lainnya



"""Output env: PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6,max_split_size_mb:128 merupakan konfirmasi bahwa perintah %env telah berhasil dijalankan, dan environment variable PYTORCH_CUDA_ALLOC_CONF sudah diatur sesuai dengan konfigurasi yang diberikan. Environment variable ini digunakan oleh PyTorch untuk mengatur perilaku pengelolaan memori GPU, khususnya dalam hal threshold pembersihan memori (garbage collection) dan ukuran maksimum alokasi blok memori. Nilai garbage_collection_threshold:0.6 berarti proses pembersihan memori akan dipicu saat penggunaan memori GPU mencapai 60%, sementara max_split_size_mb:128 membatasi ukuran maksimal blok memori yang dialokasikan menjadi 128 MB per permintaan. Output ini sendiri tidak menunjukkan proses perubahan memori secara langsung, melainkan hanya memberitahu bahwa konfigurasi telah diterapkan di lingkungan runtime. Agar pengaturan ini benar-benar mulai berpengaruh saat PyTorch digunakan, runtime perlu di-restart terlebih dahulu, karena PyTorch hanya membaca environment variable ini saat proses awal impor library. Tanpa restart, konfigurasi ini tidak akan diterapkan secara efektif pada sesi yang sedang berjalan.

## IMPORT LIBRARIES

Kode berikut ini berfungsi sebagai tahap awal dalam menyiapkan lingkungan pemrograman untuk proses pemrosesan data dan pelatihan model deep learning berbasis gambar atau video menggunakan framework PyTorch. Beberapa library standar diimpor untuk mendukung berbagai fungsi. Library os digunakan untuk pengelolaan sistem file, seperti navigasi direktori dan manipulasi file. Library pickle memungkinkan penyimpanan dan pemuatan objek Python dalam format biner, yang berguna untuk menyimpan dataset yang telah diproses maupun model yang telah dilatih. Library random digunakan untuk menghasilkan bilangan acak atau melakukan proses acak, seperti membagi dataset secara acak. Library cv2 dari OpenCV dimanfaatkan untuk membaca, memproses, dan memanipulasi data citra atau video. Library numpy berperan penting dalam pengolahan array numerik yang efisien, yang menjadi dasar representasi data dalam proses deep learning. Framework PyTorch diimpor melalui torch, sementara modul torch.utils.data menyediakan kelas Dataset dan DataLoader yang memudahkan pengelolaan data dalam proses training, serta fungsi random_split yang mempermudah pembagian dataset menjadi data latih, validasi, dan uji. Library torchvision.transforms digunakan untuk melakukan transformasi data citra, seperti normalisasi, augmentasi, dan konversi ke format tensor. Selain itu, library glob dimanfaatkan untuk pencarian file berdasarkan pola tertentu, sehingga proses pemuatan dataset citra menjadi lebih efisien. Library tqdm digunakan untuk menampilkan progress bar saat proses iteratif berjalan, misalnya saat memuat data atau melakukan training model, guna mempermudah pemantauan proses. Terakhir, perintah print(torch.cuda.is_available()) digunakan untuk memeriksa ketersediaan perangkat GPU (CUDA) pada sistem. Jika GPU tersedia, proses pelatihan model dapat dipercepat secara signifikan dengan memanfaatkan akselerasi GPU. Secara keseluruhan, kode ini merupakan langkah inisialisasi penting dalam pipeline proyek deep learning yang mempersiapkan berbagai komponen fungsional dan mengecek kesiapan perangkat keras.
"""

import os
import pickle
import random
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from glob import glob
from tqdm import tqdm

print(torch.cuda.is_available())

"""## PREVIEW 20 RANDOM VIDEOS (1st Frame of Each)

Kode berikut ini digunakan untuk menampilkan pratinjau (preview) acak dari video yang terdapat di dalam dataset UCF50, dalam bentuk gambar frame pertama dari masing-masing video. Pertama, library matplotlib.pyplot diimpor sebagai plt untuk menampilkan visualisasi gambar. Kemudian, dibuat sebuah kanvas gambar berukuran besar (20, 20) inci menggunakan plt.figure(figsize=(20, 20)) agar tampilan video yang dipilih terlihat jelas. Selanjutnya, kode mengambil seluruh nama kelas yang terdapat dalam direktori /content/UCF50 menggunakan os.listdir. Dari daftar kelas tersebut, dipilih secara acak sebanyak 20 kelas menggunakan fungsi random.sample. Untuk masing-masing kelas yang terpilih, kode memilih secara acak satu file video dari dalam folder kelas tersebut. Kemudian, dengan menggunakan cv2.VideoCapture, kode membaca frame pertama dari video yang dipilih. Jika frame berhasil dibaca, frame diubah dari format warna BGR (standar OpenCV) menjadi RGB (standar tampilan matplotlib) agar warna tampil dengan benar. Pada frame tersebut, ditambahkan nama kelas sebagai teks di bagian atas gambar menggunakan cv2.putText. Setelah itu, frame ditampilkan pada subplot ke-counter dalam grid 5 x 4 (20 gambar total). Jika frame gagal dibaca, maka akan dicetak pesan kesalahan ke layar. Di akhir proses, plt.tight_layout() dipanggil agar tata letak gambar lebih rapih dan plt.show() digunakan untuk menampilkan seluruh grid gambar. Secara keseluruhan, kode ini berfungsi untuk melakukan visual eksplorasi awal terhadap dataset video dengan menampilkan representasi visual dari beberapa kelas secara acak, sehingga membantu pengguna memahami karakteristik dan variasi data dalam dataset UCF50.
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(20, 20))

# Get class names from the dataset directory
all_classes_names = os.listdir('/content/UCF50')

# Select 20 random classes
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterate and preview videos
for counter, random_index in enumerate(random_range, 1):
    selected_class_Name = all_classes_names[random_index]
    video_files_names_list = os.listdir(f'/content/UCF50/{selected_class_Name}')
    selected_video_file_name = random.choice(video_files_names_list)

    video_reader = cv2.VideoCapture(f'/content/UCF50/{selected_class_Name}/{selected_video_file_name}')
    success, bgr_frame = video_reader.read()
    video_reader.release()

    if success:
        rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)
        cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)
        plt.subplot(5, 4, counter)
        plt.imshow(rgb_frame)
        plt.axis('off')
    else:
        print(f"Gagal membaca frame dari video: {selected_video_file_name} di kelas {selected_class_Name}")

plt.tight_layout()
plt.show()

"""## SET CONSTANTS

Kode berikut ini berfungsi untuk mendefinisikan beberapa parameter penting yang akan digunakan dalam proses pemrosesan dan pelatihan model pada dataset video UCF50. Variabel PATH_DATASET_DIR menyimpan lokasi direktori utama dataset, yaitu folder tempat seluruh data video disimpan. Parameter HEIGHT dan WIDTH menetapkan ukuran dimensi frame video yang akan diproses, di mana setiap frame akan diubah ukurannya menjadi 64x64 piksel untuk memudahkan pengolahan dan mempercepat komputasi. Variabel SEQUENCE_LENGTH menunjukkan jumlah frame berturut-turut yang akan diambil sebagai satu urutan (sequence) untuk dianalisis oleh model, dalam hal ini sebanyak 10 frame. batch_size menentukan jumlah urutan data yang akan diproses sekaligus dalam satu iterasi selama pelatihan, yaitu 4 urutan per batch, yang berpengaruh pada efisiensi pelatihan dan penggunaan memori. Terakhir, num_workers mengatur jumlah proses paralel yang digunakan untuk memuat data, dengan nilai 2 menunjukkan bahwa dua proses akan berjalan bersamaan untuk mempercepat pengambilan data dari disk. Secara keseluruhan, kode ini menetapkan konfigurasi dasar yang penting untuk mengatur cara data video diproses dan diolah dalam pipeline pelatihan model deep learning.
"""

PATH_DATASET_DIR = "/content/UCF50"
HEIGHT, WIDTH = 64, 64
SEQUENCE_LENGTH = 10
batch_size = 32
num_workers = 2

"""## DEFINE TRANSFORM

Kode berikut ini bertujuan untuk mendefinisikan rangkaian transformasi yang akan diterapkan pada setiap frame video sebelum digunakan dalam pelatihan model deep learning. Pertama, modul transforms dari library torchvision diimpor, yang menyediakan berbagai fungsi untuk memanipulasi citra secara efisien. Variabel HEIGHT dan WIDTH menetapkan ukuran target frame, yaitu 64x64 piksel. Selanjutnya, transform dibuat menggunakan transforms.Compose yang menggabungkan beberapa transformasi secara berurutan. Pertama, frame yang awalnya berupa array atau tensor diubah menjadi objek gambar PIL dengan ToPILImage(). Kemudian, gambar tersebut diubah ukurannya (resize) menjadi 64x64 piksel sesuai yang telah ditentukan. Setelah itu, gambar dikonversi menjadi tensor dengan ToTensor(), sehingga bisa diproses oleh PyTorch. Terakhir, tensor tersebut dinormalisasi dengan menggunakan nilai rata-rata (mean) dan standar deviasi (std) tertentu untuk setiap saluran warna RGB, yang merupakan nilai umum untuk model pretrained pada dataset ImageNet. Normalisasi ini penting agar nilai piksel berada dalam rentang yang sesuai sehingga pelatihan model menjadi lebih stabil dan konvergen lebih cepat. Secara keseluruhan, kode ini menyiapkan pipeline transformasi standar yang memastikan setiap frame video dipersiapkan secara konsisten dan optimal sebelum dimasukkan ke dalam model.
"""

from torchvision import transforms

HEIGHT, WIDTH = 64, 64

transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((HEIGHT, WIDTH)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

"""## COLLECT VIDEO PATHS AND LABELS

Kode berikut ini berfungsi untuk mencari dan mengumpulkan semua file video dengan format .avi yang terdapat di dalam folder dataset UCF50 beserta seluruh subfoldernya secara rekursif. Dengan menggunakan modul glob, fungsi glob() dipanggil dengan pola path yang menggabungkan direktori utama dataset (PATH_DATASET_DIR) dan wildcard **/*.avi, yang berarti mencari semua file berekstensi .avi di dalam folder tersebut dan seluruh subfolder tanpa batas kedalaman. Parameter recursive=True memastikan pencarian dilakukan secara menyeluruh ke dalam subdirektori. Hasil pencarian berupa daftar path lengkap dari semua file video yang ditemukan disimpan dalam variabel videos. Terakhir, kode mencetak jumlah total file video yang berhasil ditemukan untuk memberikan informasi mengenai ukuran dataset yang akan digunakan. Kode ini penting sebagai langkah awal dalam pemrosesan data untuk memastikan semua video dalam dataset telah terdeteksi dan siap diolah.
"""

from glob import glob

PATH_DATASET_DIR = "/content/UCF50"
videos = glob(f"{PATH_DATASET_DIR}/**/*.avi", recursive=True)
print(f"Total videos found: {len(videos)}")

"""## CREATE LABEL MAPPING AND SAVE TO PICKLE

Kode berikut ini berfungsi untuk membuat dan menyimpan pemetaan label kelas ke indeks numerik dari dataset video yang tersimpan di folder /content/UCF50. Pertama, modul os dan pickle diimpor untuk operasi file dan penyimpanan data dalam format biner. Variabel PATH_DATASET_DIR menyimpan lokasi folder utama dataset. Selanjutnya, kode mengambil semua nama folder di dalam direktori dataset menggunakan os.listdir dan memfilter hanya yang merupakan direktori (folder), yang diasumsikan sebagai label atau kelas video. Daftar folder ini kemudian diurutkan secara alfabetis dengan sorted(). Setelah itu, dibuat sebuah dictionary lab2idx yang memetakan setiap nama label ke sebuah indeks unik yang dihasilkan secara berurutan menggunakan enumerate. Tujuan pemetaan ini adalah untuk mengubah label berupa string menjadi angka yang dapat digunakan model machine learning. Terakhir, dictionary tersebut disimpan ke dalam sebuah file bernama real_lab2idx.pickle menggunakan modul pickle dengan mode tulis biner (wb). File ini nantinya dapat dibaca kembali untuk memudahkan proses pelabelan selama pelatihan atau evaluasi model. Secara keseluruhan, kode ini membantu dalam menyiapkan data label agar dapat diproses oleh algoritma pembelajaran mesin secara efisien dan konsisten.
"""

import os
import pickle

# Path dataset Anda
PATH_DATASET_DIR = "/content/UCF50"

# Ambil semua nama folder (label)
label_dirs = sorted([d for d in os.listdir(PATH_DATASET_DIR) if os.path.isdir(os.path.join(PATH_DATASET_DIR, d))])

# Buat mapping
lab2idx = {label: idx for idx, label in enumerate(label_dirs)}

# Simpan ke file
with open("real_lab2idx.pickle", "wb") as f:
    pickle.dump(lab2idx, f)

"""## DEFINE DATASET CLASS

Kode dataset.py ini berisi definisi kelas VideoDataset, yang berfungsi sebagai custom dataset PyTorch untuk memuat video dari filesystem, mengekstrak frame secara selektif, serta mengembalikan pasangan data berupa tensor frame dan label dalam format numerik. Kelas ini sangat berguna dalam skenario deep learning yang memerlukan input sekuensial berupa frame video, misalnya untuk klasifikasi aksi (action recognition), deteksi emosi dari video, atau model-model temporal seperti CNN-LSTM dan 3D-CNN. Dengan mewarisi kelas Dataset dari torch.utils.data, VideoDataset dapat digunakan secara langsung dalam DataLoader untuk pelatihan batch-wise yang efisien.

Pada fungsi inisialisasi (__init__), parameter penting seperti video_paths, transform, num_videos_per_class, dan sequence_length disiapkan. video_paths adalah daftar path file video yang akan dimuat, sementara transform dapat berupa pipeline augmentasi atau preprocessing. File pickle real_lab2idx.pickle dimuat untuk menerjemahkan label string (biasanya nama folder tempat video berada) menjadi label numerik. Selain itu, terdapat fitur opsional num_videos_per_class yang membatasi jumlah video yang diambil dari masing-masing kelas, berguna untuk mengatasi ketidakseimbangan data atau mempercepat pelatihan.

Bagian filtering berdasarkan kelas dilakukan dengan mengekstrak nama folder induk dari setiap video sebagai labelnya, lalu mengelompokkan video per label. Jika parameter num_videos_per_class diaktifkan, maka akan dipilih sejumlah video acak dari setiap kelas (menggunakan seed tetap untuk reprodusibilitas) sebelum dimasukkan ke dataset. Jika tidak, semua path video akan digunakan tanpa pembatasan. Hal ini memberikan fleksibilitas dalam eksperimen machine learning, terutama saat menguji performa model dengan subset data yang terbatas.

Metode __getitem__ bertanggung jawab untuk mengambil satu sample (sebuah video) berdasarkan indeks tertentu. Label diambil dari nama folder induk, lalu video diproses melalui metode read_video. Di sini, sejumlah frame ditarik secara merata dari durasi video menggunakan OpenCV. Proses ini menggunakan teknik sampling dengan interval tertentu (berdasarkan panjang video dan sequence_length) agar distribusi informasi dari seluruh video tetap representatif. Setelah frame diperoleh, jika transform disediakan (misalnya konversi ke tensor, resize, normalisasi), transformasi diterapkan ke setiap frame.

Metode read_video berfungsi untuk membuka file video dan mengekstrak sejumlah frame yang telah ditentukan. Ia menghitung jumlah total frame dalam video, menentukan jarak antar frame yang akan diambil (window), dan mengekstrak frame pada interval tersebut. Konversi warna dari BGR (default OpenCV) ke RGB dilakukan agar kompatibel dengan kebanyakan model deep learning yang mengharuskan format warna standar. Setelah frame dikumpulkan, hasilnya dikembalikan sebagai array NumPy untuk diproses lebih lanjut di __getitem__.

Terakhir, fungsi tambahan get_all_labels() disediakan untuk mengambil semua label dalam dataset dalam bentuk numerik. Fungsi ini berguna untuk analisis distribusi label, evaluasi model, atau visualisasi data. Secara keseluruhan, VideoDataset menyediakan pipeline modular dan efisien untuk membaca, memproses, dan menyajikan data video dalam format yang sesuai untuk pelatihan model berbasis PyTorch. Struktur ini juga fleksibel dan dapat diperluas untuk mendukung anotasi tambahan seperti bounding box atau keypoint jika digunakan dalam tugas yang lebih kompleks.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile dataset.py
# import os
# import numpy as np
# import pickle
# import torch
# from torch.utils.data import Dataset
# import cv2
# 
# class VideoDataset(Dataset):
#     def __init__(self, video_paths, transform=None, num_videos_per_class=None, sequence_length=10):
#         self.transform = transform
#         self.sequence_length = sequence_length
# 
#         with open("real_lab2idx.pickle", 'rb') as f:
#             self.lab2idx = pickle.load(f)
# 
#         # Filter berdasarkan jumlah video per kelas
#         if num_videos_per_class:
#             self.paths = []
#             labels = set([os.path.basename(os.path.dirname(p)) for p in video_paths])
#             for label in labels:
#                 vids = [p for p in video_paths if os.path.basename(os.path.dirname(p)) == label]
#                 np.random.seed(1)
#                 vids = np.random.choice(np.array(vids), num_videos_per_class, replace=False)
#                 self.paths.extend(vids)
#         else:
#             self.paths = video_paths
# 
#     def __len__(self):
#         return len(self.paths)
# 
#     def __getitem__(self, idx):
#         video_path = self.paths[idx]
#         label = os.path.basename(os.path.dirname(video_path))
#         frames = self.read_video(video_path)
# 
#         if self.transform:
#             frames = torch.stack([self.transform(frame) for frame in frames])
#         else:
#             frames = torch.tensor(frames)
# 
#         return frames, self.lab2idx[label]
# 
#     def read_video(self, video_path):
#         cap = cv2.VideoCapture(video_path)
#         frames = []
#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         window = max(total_frames // self.sequence_length, 1)
# 
#         for i in range(self.sequence_length):
#             cap.set(cv2.CAP_PROP_POS_FRAMES, i * window)
#             ret, frame = cap.read()
#             if not ret:
#                 break
#             frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#             frames.append(frame)
# 
#         cap.release()
#         return np.array(frames)
# 
#     def get_all_labels(self):
#         return [self.lab2idx[os.path.basename(os.path.dirname(p))] for p in self.paths]

"""## MENAMPILKAN DAFTAR LABEL KELAS UCF50 (OPTIONAL)

Kode berikut ini bertujuan untuk membaca dan menampilkan daftar label kelas yang ada dalam dataset video UCF50. Dengan mengakses folder utama dataset (PATH_DATASET_DIR), kode mengambil semua nama folder yang berada di dalamnya, karena setiap folder diasumsikan mewakili satu kelas atau label video. Selanjutnya, daftar nama folder tersebut diurutkan secara alfabetis untuk memudahkan pengelolaan. Kemudian, kode mencetak semua label kelas beserta indeks urutannya ke layar sebagai informasi yang berguna untuk pemahaman struktur dataset dan persiapan proses pelabelan pada tahap selanjutnya dalam pengolahan data. Ini merupakan langkah awal penting untuk mengenali dan mengorganisasi data sebelum dilakukan pelatihan model machine learning atau deep learning.
"""

import os

# Path ke direktori dataset
PATH_DATASET_DIR = "/content/UCF50"  # atau sesuaikan dengan lokasi datasetmu

# Ambil nama-nama folder yang merupakan label
class_names = sorted([d for d in os.listdir(PATH_DATASET_DIR) if os.path.isdir(os.path.join(PATH_DATASET_DIR, d))])

# Tampilkan semua label
print("Label asli (class_names):")
for i, name in enumerate(class_names):
    print(f"{i}: {name}")

"""## LOAD DATASET

Kode berikut ini melakukan beberapa hal penting terkait penggunaan kelas VideoDataset yang sebelumnya sudah didefinisikan untuk memuat dan mengelola dataset video. Pertama, sebuah objek dataset dibuat dari kelas VideoDataset dengan memasukkan daftar path video (videos), transformasi gambar (transform) yang akan diterapkan pada setiap frame video, dan membatasi jumlah video per kelas menjadi maksimal 100 untuk menjaga keseimbangan data. Setelah dataset dibuat, kode mencetak ukuran dataset atau jumlah total video yang ada di objek dataset menggunakan len(dataset). Kemudian, kode juga menampilkan bentuk (dimensi) dari contoh item pertama pada dataset, yaitu tensor yang berisi urutan frame video setelah transformasi diterapkan. Terakhir, kode mencetak label indeks dari contoh video pertama, yang merepresentasikan kelas video tersebut dalam format numerik. Dengan demikian, kode ini bertujuan untuk memastikan bahwa dataset sudah berhasil dibuat dan untuk memberikan gambaran awal mengenai struktur data dan label yang akan digunakan dalam pelatihan model.
"""

from dataset import VideoDataset

dataset = VideoDataset(videos, transform=transform, num_videos_per_class=100)
print(f"Dataset size: {len(dataset)}")
print(f"Sample item shape: {dataset[0][0].shape}")
print(f"Sample label index: {dataset[0][1]}")

"""Output ini menunjukkan hasil dari inisialisasi dan pemeriksaan awal pada objek dataset yang dibuat dari kelas VideoDataset.

Pertama, Dataset size: 5000 berarti dataset berisi total 5000 video yang sudah dipilih, dengan pembatasan maksimal 100 video per kelas, sehingga jumlah video keseluruhan mencapai 5000.

Kedua, Sample item shape: torch.Size([10, 3, 64, 64]) menginformasikan bahwa setiap sampel data berupa tensor dengan ukuran [10, 3, 64, 64]. Ini berarti setiap video diwakili oleh 10 frame (urutan frame), setiap frame memiliki 3 saluran warna (RGB), dan resolusi frame sudah diubah menjadi 64x64 piksel. Format ini sesuai dengan input yang biasanya digunakan untuk model deep learning berbasis video.

Terakhir, Sample label index: 6 menunjukkan bahwa label video contoh pertama pada dataset ini memiliki indeks kelas ke-6 dalam daftar label yang sudah didefinisikan sebelumnya. Label ini adalah representasi numerik yang memudahkan proses pelatihan dan evaluasi model.

Secara keseluruhan, output ini memastikan bahwa dataset telah terisi dengan benar, memiliki struktur data yang tepat, dan label yang sesuai untuk digunakan dalam pelatihan model pembelajaran mesin.

## SPLIT DATASET

Kode berikut ini berfungsi untuk membagi dataset video menjadi tiga subset penting untuk proses pelatihan model pembelajaran mesin: data latih (train), data validasi (validation), dan data uji (test). Pembagian ini dilakukan secara stratified, yaitu dengan mempertahankan proporsi label kelas agar distribusi kelas tetap seimbang di setiap subset.

Langkah pertama, kode mengambil semua label dari dataset tanpa harus memuat semua video secara langsung, dengan memanggil fungsi get_all_labels(). Kemudian, dengan menggunakan StratifiedShuffleSplit dari library scikit-learn, dataset dibagi menjadi dua bagian utama: 70% untuk data latih dan 30% sisanya untuk data validasi dan uji bersama-sama.

Setelah itu, bagian 30% tersebut dibagi lagi menjadi dua bagian, yaitu 20% untuk data validasi dan 10% untuk data uji, juga secara stratified agar distribusi label tetap proporsional. Karena hasil pembagian ini didapat dalam indeks relatif, kode melakukan pemetaan ulang indeks subset validasi dan uji ke indeks asli dataset.

Setelah indeks untuk train, validation, dan test didapat, kode membuat objek subset dataset menggunakan torch.utils.data.Subset, yang memungkinkan mengambil hanya data yang sesuai dari dataset utama tanpa perlu duplikasi data.

Terakhir, setiap subset dataset dibungkus dalam objek DataLoader PyTorch untuk memudahkan pengambilan batch data secara efisien selama pelatihan dan evaluasi model. DataLoader ini juga diatur dengan parameter seperti ukuran batch, pengacakan data untuk train, dan jumlah proses worker untuk mempercepat loading data.

Secara keseluruhan, kode ini bertujuan untuk melakukan pembagian data yang terstruktur dan efisien, menjaga keseimbangan kelas, serta menyiapkan data dalam format yang optimal untuk pelatihan deep learning.
"""

from sklearn.model_selection import StratifiedShuffleSplit
import numpy as np
import torch
from torch.utils.data import DataLoader

# Ambil label dari dataset tanpa load semua video
all_labels = dataset.get_all_labels()

# Stratified split: train (70%), val (20%), test (10%)
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
train_idx, temp_idx = next(splitter.split(np.zeros(len(all_labels)), all_labels))

# Split val/test dari temp_idx (val: 20%, test: 10%)
temp_labels = [all_labels[i] for i in temp_idx]
splitter = StratifiedShuffleSplit(n_splits=1, test_size=2/3, random_state=42)
val_idx, test_idx = next(splitter.split(np.zeros(len(temp_labels)), temp_labels))

# Mapping indeks val/test ke indeks asli dataset
val_idx = [temp_idx[i] for i in val_idx]
test_idx = [temp_idx[i] for i in test_idx]

# Buat subset dataset
train_dataset = torch.utils.data.Subset(dataset, train_idx)
val_dataset = torch.utils.data.Subset(dataset, val_idx)
test_dataset = torch.utils.data.Subset(dataset, test_idx)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=num_workers)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers)

"""## SAVE & LOAD SPLITS (OPTIONAL)

Kode berikut ini berfungsi untuk menyimpan dan memuat ulang objek dataset dan subset dataset yang telah dibuat menggunakan PyTorch. Fungsi torch.save() digunakan untuk menyimpan objek Python apa pun (dalam hal ini dataset dan subsetnya) ke dalam file dengan ekstensi .pth. Ini sangat berguna agar proses pembagian dan persiapan dataset tidak perlu diulang dari awal setiap kali kita menjalankan ulang program, sehingga menghemat waktu dan sumber daya.

Bagian pertama kode menyimpan empat objek: dataset lengkap (dataset), serta tiga subsetnya yaitu data latih (train_dataset), data validasi (val_dataset), dan data uji (test_dataset). Setiap objek disimpan ke file berbeda sesuai namanya.

Bagian kedua (yang dikomentari) menunjukkan bagaimana cara memuat kembali objek-objek tersebut dari file menggunakan torch.load(). Dengan memuat ulang dataset yang sudah disimpan, pengguna bisa langsung menggunakan dataset tersebut tanpa harus mengulangi proses pembagian atau pemrosesan data yang sebelumnya sudah dilakukan.

Secara singkat, kode ini bertujuan untuk menyimpan kondisi dataset agar dapat digunakan kembali di lain waktu dengan mudah dan efisien.
"""

torch.save(dataset, "dataset.pth")
torch.save(train_dataset, "train_dataset.pth")
torch.save(val_dataset, "val_dataset.pth")
torch.save(test_dataset, "test_dataset.pth")

# Untuk load ulang
# dataset = torch.load("dataset.pth")
# train_dataset = torch.load("train_dataset.pth")
# val_dataset = torch.load("val_dataset.pth")
# test_dataset = torch.load("test_dataset.pth")

"""## DEFINE CONVLSTMCELL & CONVLSTM MODULE (MODEL.PY)

Kode berikut ini adalah implementasi lengkap sebuah model deep learning khusus untuk pengolahan video menggunakan PyTorch, yang menggabungkan layer ConvLSTM (Convolutional LSTM) dan pooling campuran (MixedPool3d) untuk klasifikasi video. Berikut penjelasan setiap bagiannya:

1. MixedPool3d
Layer pooling yang menggabungkan MaxPooling3d dan AveragePooling3d secara terberat (weighted sum).

self.w adalah parameter yang dapat dilatih untuk mengatur bobot antara maxpool dan avgpool.

Tujuan: Mengambil fitur penting dengan lebih fleksibel daripada hanya max atau avg pooling biasa.

2. ConvLSTMCell
Sel dasar ConvLSTM, yaitu LSTM yang mengaplikasikan konvolusi di input dan hidden state daripada operasi matriks biasa.

Input: gabungan tensor input dan hidden state.

Output: hidden state dan cell state baru untuk tiap timestep.

Tujuan: Memproses data berurutan (sequence video frame) dengan mempertahankan informasi spasial (konvolusi).

3. ConvLSTM
Layer ConvLSTM berlapis-lapis (bisa lebih dari satu lapisan).

Input berupa batch video frames dengan dimensi (batch, time, channel, height, width).

Mengiterasi tiap frame secara berurutan dan memproses dengan beberapa ConvLSTMCell.

Output: sequence hidden state dari timestep terakhir dan seluruh hidden & cell states.

4. ClassificationModelConvLSTM
Model utama klasifikasi video.

Memiliki dua blok ConvLSTM berturut-turut, masing-masing diikuti dengan MixedPool3d dan dropout 3D.

Setelah blok ConvLSTM, hasilnya diproses dengan fully connected layer untuk klasifikasi ke sejumlah kelas (default 50 kelas).

Menggunakan fungsi _get_fc_input_dim untuk menghitung dimensi input ke fully connected layer secara otomatis dengan dummy input.

Input data video harus berukuran (B, T, C, 64, 64) — 64x64 pixel tiap frame, T=jumlah frame per video.

Output berupa skor logits kelas video.

Tujuan utama kode ini:
Membangun model deep learning yang mampu menangkap fitur spasial dan temporal dari video untuk klasifikasi, dengan menggabungkan kekuatan konvolusi dan LSTM secara efisien.

Kegunaan:
Model ini cocok untuk dataset video dengan banyak kelas (seperti dataset aksi manusia, video event detection, dsb).

Menggunakan ConvLSTM agar informasi waktu dan ruang diproses sekaligus.

MixedPool3d memberikan pooling yang adaptif dan lebih baik dibanding pooling biasa.

Dropout mencegah overfitting.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Create the model.py file
# %%writefile model.py
# # model.py
# 
# import torch.nn.functional as F
# import torch
# import torch.nn as nn
# 
# class MixedPool3d(nn.Module):
#     def __init__(self, kernel_size, stride=None, padding=0):
#         super(MixedPool3d, self).__init__()
#         self.maxpool = nn.MaxPool3d(kernel_size, stride=stride, padding=padding)
#         self.avgpool = nn.AvgPool3d(kernel_size, stride=stride, padding=padding)
#         self.w = nn.Parameter(torch.tensor(0.5))
# 
#     def forward(self, x):
#         max_out = self.maxpool(x)
#         avg_out = self.avgpool(x)
#         w = torch.clamp(self.w, 0.0, 1.0)
#         return w * max_out + (1 - w) * avg_out
# 
# class ConvLSTMCell(nn.Module):
#     def __init__(self, input_dim, hidden_dim, kernel_size):
#         super(ConvLSTMCell, self).__init__()
#         padding = kernel_size // 2
#         self.conv = nn.Conv2d(in_channels=input_dim + hidden_dim,
#                               out_channels=4 * hidden_dim,
#                               kernel_size=kernel_size,
#                               padding=padding)
#         self.hidden_dim = hidden_dim
# 
#     def forward(self, x, h, c):
#         combined = torch.cat([x, h], dim=1)
#         conv_output = self.conv(combined)
#         cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_output, 4, dim=1)
#         i = torch.sigmoid(cc_i)
#         f = torch.sigmoid(cc_f)
#         o = torch.sigmoid(cc_o)
#         g = torch.tanh(cc_g)
#         c_next = f * c + i * g
#         h_next = o * torch.tanh(c_next)
#         return h_next, c_next
# 
# class ConvLSTM(nn.Module):
#     def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=True):
#         super(ConvLSTM, self).__init__()
#         self.num_layers = num_layers
#         self.hidden_dim = hidden_dim
#         self.batch_first = batch_first
#         self.cells = nn.ModuleList([
#             ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim, kernel_size)
#             for i in range(num_layers)
#         ])
# 
#     def forward(self, x):
#         if self.batch_first:
#             b, t, c, h, w = x.size()
#         else:
#             t, b, c, h, w = x.size()
#             x = x.permute(1, 0, 2, 3, 4)
#         h_t = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]
#         c_t = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]
#         outputs = []
#         for time in range(t):
#             input_ = x[:, time, :, :, :]
#             for i, cell in enumerate(self.cells):
#                 h_t[i], c_t[i] = cell(input_, h_t[i], c_t[i])
#                 input_ = h_t[i]
#             outputs.append(h_t[-1])
#         output_seq = torch.stack(outputs, dim=1)
#         return output_seq, (h_t, c_t)
# 
# class ClassificationModelConvLSTM(nn.Module):
#     def __init__(self, input_dim=3, hidden_size=128, num_layers=1, num_classes=50, input_size=(64, 64), dropout_rate=0.5):
#         super(ClassificationModelConvLSTM, self).__init__()
# 
#         self.convlstm1 = ConvLSTM(input_dim=input_dim, hidden_dim=hidden_size, kernel_size=3,
#                                   num_layers=num_layers, batch_first=True)
#         self.pool1 = MixedPool3d(kernel_size=(1, 2, 2))
#         self.dropout1 = nn.Dropout3d(dropout_rate)
# 
#         self.convlstm2 = ConvLSTM(input_dim=hidden_size, hidden_dim=hidden_size, kernel_size=3,
#                                   num_layers=num_layers, batch_first=True)
#         self.pool2 = MixedPool3d(kernel_size=(1, 2, 2))
#         self.dropout2 = nn.Dropout3d(dropout_rate)
# 
#         self.fc_input_dim = self._get_fc_input_dim(input_dim, input_size, hidden_size)
#         self.dropout_fc = nn.Dropout(dropout_rate)
#         self.fc = nn.Linear(self.fc_input_dim, num_classes)
# 
#     def _get_fc_input_dim(self, input_dim, input_size, hidden_size):
#         dummy_input = torch.zeros(1, 10, input_dim, input_size[0], input_size[1])  # (B, T=10, C, H, W)
#         x, _ = self.convlstm1(dummy_input)
#         x = self.pool1(x)
#         x = self.dropout1(x) # Include dropout in dimension calculation
#         x, _ = self.convlstm2(x)
#         x = self.pool2(x)
#         x = self.dropout2(x) # Include dropout in dimension calculation
#         x = x[:, -1]
#         return x.view(x.size(0), -1).shape[1]
# 
# 
#     def forward(self, x):  # x: (B, T, C, H, W)
#         assert x.shape[3:] == (64, 64), f"Expected input size (HxW) = (64,64), but got {x.shape[3:]}"
# 
#         x, _ = self.convlstm1(x)
#         x = self.pool1(x)
#         x = self.dropout1(x)
# 
#         x, _ = self.convlstm2(x)
#         x = self.pool2(x)
#         x = self.dropout2(x)
# 
#         x = x[:, -1]
#         x = x.view(x.size(0), -1)
#         x = self.dropout_fc(x)
#         x = self.fc(x)
#         return x

"""## UTILS.PY

Kode berikut ini mendefinisikan sebuah fungsi bernama visualize_history di dalam file utils.py yang digunakan untuk memvisualisasikan grafik hasil pelatihan model machine learning atau deep learning. Fungsi ini menerima parameter history yang berisi data metrik pelatihan dan validasi, seperti akurasi (acc) dan loss (kerugian), dalam bentuk dictionary.

Ketika fungsi dijalankan, jika metrik acc diminta, maka fungsi akan membuat subplot pertama yang menampilkan perubahan nilai akurasi pada data pelatihan (train_acc) dan validasi (val_acc) selama beberapa epoch. Grafik ini berfungsi untuk memantau seberapa baik model belajar dari waktu ke waktu.

Selanjutnya, jika metrik loss diminta, subplot kedua akan menampilkan perubahan nilai loss pada data pelatihan (train_loss) dan validasi (val_loss) selama epoch yang sama. Loss yang menurun biasanya menandakan model semakin baik dalam menyesuaikan diri dengan data.

Fungsi ini menggunakan matplotlib untuk menggambar grafik dan menampilkan legenda, label sumbu, dan judul agar mudah dibaca. Tujuannya adalah memberikan gambaran visual yang jelas tentang performa model selama proses pelatihan sehingga dapat digunakan untuk evaluasi dan perbaikan model.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile utils.py
# import matplotlib.pyplot as plt
# 
# def visualize_history(history, metrics=['acc', 'loss']):
#     if 'acc' in metrics:
#         plt.figure(figsize=(10,6))
#         plt.subplot(121)
#         plt.plot(range(1, len(history['train_acc']) + 1), history['train_acc'], label='train_acc', c='r')
#         plt.plot(range(1, len(history['val_acc']) + 1), history['val_acc'], label='val_acc', c='g')
#         plt.title('Accuracy')
#         plt.xlabel('Epoch')
#         plt.ylabel('Accuracy')
#         plt.legend()
# 
#     if 'loss' in metrics:
#         plt.subplot(122)
#         plt.plot(range(1, len(history['train_loss']) + 1), history['train_loss'], label='train_loss', c='r')
#         plt.plot(range(1, len(history['val_loss']) + 1), history['val_loss'], label='val_loss', c='g')
#         plt.title('Loss')
#         plt.xlabel('Epoch')
#         plt.ylabel('Loss')
#         plt.legend()
#     plt.show()

"""## PARAMETERS.PY

Kode berikut ini berisi konfigurasi atau pengaturan penting yang digunakan dalam proyek deep learning, dan disimpan dalam file bernama parameters.py. Berikut penjelasan tiap bagiannya:

Device Config:
Kode mendeteksi apakah tersedia GPU (CUDA) di perangkat yang digunakan. Jika ada, maka proses pelatihan dan inferensi model akan menggunakan GPU (cuda:0), jika tidak maka menggunakan CPU. Ini penting untuk mempercepat komputasi.

Dataset Config:
Menyimpan berbagai pengaturan terkait dataset video UCF50, seperti lokasi folder dataset (PATH_DATASET_DIR), ukuran frame video (64x64 piksel), panjang urutan frame yang digunakan (SEQUENCE_LENGTH=10), jumlah kelas target (num_classes=50), ukuran batch saat pelatihan (batch_size=32), dan jumlah pekerja (threads) yang dipakai untuk memuat data (num_workers=2).

Model Config:
Menentukan parameter untuk arsitektur model, seperti ukuran tersembunyi LSTM (hidden_size=128), jumlah lapisan LSTM (num_lstm_layers=1), jumlah channel input (3, karena video menggunakan warna RGB), serta ukuran input frame (model_input_size).

Training Config:
Mengatur nilai learning rate (learning_rate=0.0001) untuk optimisasi dan jumlah epoch pelatihan (epochs=50).

Paths untuk Penyimpanan Model:
Menentukan direktori di Google Drive untuk menyimpan bobot model yang dilatih (base_save_path). Jika direktori belum ada, maka akan dibuat otomatis.
best_model_path adalah lokasi file untuk menyimpan bobot model terbaik selama pelatihan.
last_weight_template adalah fungsi lambda yang menghasilkan path file bobot model terakhir berdasarkan nomor epoch, memudahkan penyimpanan berkala.

Loss Function:
Mendefinisikan fungsi loss yang dipakai, yaitu CrossEntropyLoss yang umum digunakan untuk masalah klasifikasi multi-kelas.

Secara keseluruhan, kode ini memusatkan berbagai pengaturan penting agar mudah digunakan dan diubah saat pengembangan model deep learning, menjaga kode utama tetap rapi dan modular.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile parameters.py
# import os
# import torch
# import torch.nn as nn
# 
# # Device config
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# 
# # Dataset config
# PATH_DATASET_DIR = "/content/UCF50"
# HEIGHT, WIDTH = 64, 64
# SEQUENCE_LENGTH = 10
# num_classes = 50
# batch_size = 32
# num_workers = 2
# 
# # Model config
# hidden_size = 128
# num_lstm_layers = 1
# model_input_dim = 3  # RGB channels
# model_input_size = (HEIGHT, WIDTH)
# 
# # Training config
# learning_rate = 1e-4
# epochs = 50
# 
# # Paths (pastikan mount Google Drive terlebih dahulu)
# base_save_path = "/content/gdrive/MyDrive/weights/"
# os.makedirs(base_save_path, exist_ok=True)
# 
# # File path untuk menyimpan model
# best_model_path = os.path.join(base_save_path, "best_model.pt")
# last_weight_template = lambda epoch: os.path.join(base_save_path, f"last_weights_epoch_{epoch}.pt")
# 
# # Loss function
# loss_fn = nn.CrossEntropyLoss()
#

"""## TRAIN.PY

Kode dalam file train.py ini berfungsi untuk melakukan proses pelatihan dan evaluasi model deep learning menggunakan PyTorch. Fungsi utama dalam kode ini adalah train, yang menjalankan pelatihan model selama sejumlah epoch yang ditentukan serta melakukan evaluasi pada data validasi untuk memantau performa model secara berkala. Selain itu, ada juga fungsi evaluate yang digunakan untuk mengukur performa model pada dataset tertentu tanpa mempengaruhi parameter model, sehingga memudahkan pengujian secara terpisah dari proses pelatihan.

Pada awal fungsi train, model dipindahkan ke device yang tersedia, biasanya GPU jika ada, agar proses pelatihan bisa berjalan lebih cepat dan efisien. Pelatihan dilakukan dalam sebuah loop berdasarkan jumlah epoch yang ditentukan. Di setiap epoch, model diatur ke mode training agar layer-layer seperti dropout dan batch normalization bekerja sesuai dengan fungsi pelatihan. Data input dan label diproses secara batch, dan proses forward pass dilakukan dengan teknik mixed precision menggunakan autocast dan GradScaler. Teknik ini meningkatkan efisiensi komputasi serta mengoptimalkan penggunaan memori pada GPU.

Selama pelatihan, kode ini menghitung loss dan akurasi secara bertahap pada data training. Model dapat menghasilkan output berdimensi tiga, seperti pada kasus LSTM, sehingga output terakhir saja yang diambil untuk perhitungan loss dan akurasi. Proses backward pass dan update parameter model dilakukan dengan bantuan GradScaler untuk menjaga kestabilan pelatihan dalam mixed precision. Selain itu, penggunaan progress bar dari tqdm membuat pemantauan proses pelatihan menjadi lebih mudah dan interaktif secara real-time.

Setelah proses pelatihan pada setiap epoch selesai, fungsi beralih ke tahap validasi. Pada tahap ini, model diubah ke mode evaluasi dan tidak dilakukan perhitungan gradien, sehingga proses validasi lebih efisien dan tidak mempengaruhi bobot model. Fungsi menghitung loss dan akurasi validasi untuk menilai performa model pada data yang belum pernah dilihat selama pelatihan. Nilai loss dan akurasi untuk training maupun validasi disimpan dalam dictionary history, yang berguna untuk analisis tren dan visualisasi performa model.

Kode ini juga memiliki mekanisme penyimpanan bobot model secara otomatis. Bobot terakhir dari setiap epoch dapat disimpan, begitu pula bobot model terbaik berdasarkan loss validasi paling rendah. Hal ini memastikan hasil pelatihan terbaik tidak hilang dan siap digunakan untuk evaluasi lebih lanjut atau deployment. Fitur ini sangat penting dalam menjaga kualitas model saat bekerja dengan dataset besar dan pelatihan yang lama.

Penggunaan scheduler learning rate pada kode ini menambah fleksibilitas dalam proses pelatihan dengan melakukan penyesuaian tingkat pembelajaran secara adaptif berdasarkan performa validasi. Scheduler membantu model untuk mencapai konvergensi yang lebih baik dan mencegah model dari stuck pada minimum lokal yang buruk. Ini menjadikan pelatihan lebih stabil dan efektif tanpa perlu pengaturan manual learning rate yang rumit.

Secara keseluruhan, kode ini adalah implementasi standar dan lengkap untuk melatih model neural network modern dengan berbagai fitur penting. Mixed precision training mempercepat pelatihan sekaligus menghemat memori GPU. Pemantauan metrik utama seperti akurasi dan loss secara otomatis memudahkan evaluasi performa. Penyimpanan bobot model terbaik dan penggunaan scheduler learning rate membuat pelatihan lebih handal dan adaptif. Fungsi evaluasi terpisah juga memudahkan pengujian performa model pada data baru secara efisien dan terstruktur.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile train.py
# import torch
# from tqdm import tqdm
# import warnings
# from sklearn.exceptions import UndefinedMetricWarning
# from torch.cuda.amp import autocast, GradScaler
# 
# warnings.filterwarnings("ignore", category=UndefinedMetricWarning)
# 
# def train(model, train_loader, val_loader, loss_fn, optimizer, epochs, device,
#           scheduler=None, save_last_weights_path=None, save_best_weights_path=None):
#     history = {'train_acc': [], 'val_acc': [], 'train_loss': [], 'val_loss': []}
#     best_val_loss = float('inf')
#     model.to(device)
#     scaler = GradScaler()
# 
#     for epoch in range(epochs):
#         model.train()
#         running_loss, correct, total = 0, 0, 0
#         loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} Training", leave=False)
# 
#         for inputs, labels in loop:
#             inputs, labels = inputs.to(device), labels.to(device)
#             optimizer.zero_grad()
# 
#             with autocast():
#                 outputs = model(inputs)
#                 if outputs.dim() == 3:
#                     outputs = outputs[:, -1, :]
#                 loss = loss_fn(outputs, labels)
# 
#             scaler.scale(loss).backward()
#             scaler.step(optimizer)
#             scaler.update()
# 
#             running_loss += loss.item() * inputs.size(0)
#             _, predicted = torch.max(outputs, 1)
#             total += labels.size(0)
#             correct += (predicted == labels).sum().item()
# 
#             loop.set_postfix(loss=loss.item(), acc=correct / total)
# 
#         train_loss = running_loss / total
#         train_acc = correct / total
# 
#         # Validation
#         model.eval()
#         val_loss, val_correct, val_total = 0, 0, 0
#         with torch.no_grad():
#             for inputs, labels in val_loader:
#                 inputs, labels = inputs.to(device), labels.to(device)
#                 outputs = model(inputs)
#                 if outputs.dim() == 3:
#                     outputs = outputs[:, -1, :]
#                 loss = loss_fn(outputs, labels)
#                 val_loss += loss.item() * inputs.size(0)
#                 _, predicted = torch.max(outputs, 1)
#                 val_total += labels.size(0)
#                 val_correct += (predicted == labels).sum().item()
# 
#         val_loss /= val_total
#         val_acc = val_correct / val_total
# 
#         history['train_loss'].append(train_loss)
#         history['train_acc'].append(train_acc)
#         history['val_loss'].append(val_loss)
#         history['val_acc'].append(val_acc)
# 
#         print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")
# 
#         if scheduler:
#             try:
#                 scheduler.step(val_loss)
#             except TypeError:
#                 scheduler.step()
# 
#         if save_last_weights_path:
#             torch.save(model.state_dict(), save_last_weights_path(epoch + 1))
# 
#         if save_best_weights_path and val_loss < best_val_loss:
#             best_val_loss = val_loss
#             torch.save(model.state_dict(), save_best_weights_path)
#             print(f"Best model saved at epoch {epoch+1}")
# 
#         torch.cuda.empty_cache()
# 
#     return model, history
# 
# def evaluate(model, loader, loss_fn, device='cuda', verbose=False):
#     device = torch.device(device if torch.cuda.is_available() else 'cpu')
#     model.to(device)
#     model.eval()
#     total_loss, total_correct, total_samples = 0, 0, 0
# 
#     with torch.no_grad():
#         if verbose:
#             loader = tqdm(loader, desc='Evaluating', ncols=100)
#         for inputs, labels in loader:
#             inputs, labels = inputs.to(device), labels.to(device)
#             outputs = model(inputs)
#             if outputs.dim() == 3:
#                 outputs = outputs[:, -1, :]
#             loss = loss_fn(outputs, labels)
#             total_loss += loss.item() * inputs.size(0)
#             _, preds = torch.max(outputs, 1)
#             total_correct += (preds == labels).sum().item()
#             total_samples += labels.size(0)
# 
#     avg_loss = total_loss / total_samples if total_samples > 0 else 0
#     accuracy = total_correct / total_samples if total_samples > 0 else 0
#     return avg_loss, accuracy
#

"""## MAIN.PY

Kode main.py ini merupakan skrip utama yang menjalankan pipeline pelatihan model deep learning pada dataset video menggunakan PyTorch dan beberapa library pendukung lainnya.

Pada awalnya, kode mengimpor berbagai modul yang dibutuhkan, seperti model ClassificationModelConvLSTM, fungsi pelatihan dan evaluasi (train dan evaluate), serta fungsi untuk visualisasi hasil (visualize_history). Selain itu, modul untuk pengolahan data seperti DataLoader, transformasi gambar dari torchvision.transforms, dan pembagian data stratified dari sklearn.model_selection juga diimpor. Jika dijalankan di Google Colab, terdapat bagian kode untuk mounting Google Drive agar data dapat diakses.

Selanjutnya, transformasi data video didefinisikan ulang untuk mengubah gambar video menjadi tensor dengan ukuran dan normalisasi standar yang umum digunakan pada model berbasis RGB, termasuk konversi ke PIL Image dan resize ke ukuran input model. Dataset video dimuat menggunakan kelas VideoDataset dengan daftar video dari direktori UCF50, dan labelnya digunakan untuk melakukan pembagian data secara stratified agar distribusi kelas seimbang pada subset training, validasi, dan testing.

Pembagian data dilakukan dengan proporsi 70% untuk training dan 30% untuk gabungan validation serta testing, yang kemudian 30% tersebut dibagi lagi menjadi 20% validation dan 10% testing secara proporsional.

Untuk setiap subset dataset ini dibuat DataLoader agar data dapat di-load secara batch saat pelatihan dan evaluasi, dengan opsi shuffle hanya aktif pada data training dan jumlah worker disesuaikan agar proses loading data lebih cepat.

Model ConvLSTM kemudian diinisialisasi dengan parameter dimensi input, ukuran hidden layer, jumlah layer LSTM, jumlah kelas, dan ukuran input spasial, lalu dipindahkan ke device yang sesuai (CPU atau GPU). Optimizer Adam digunakan dengan penambahan regularisasi weight_decay untuk mencegah overfitting, dan scheduler ReduceLROnPlateau disiapkan untuk menyesuaikan learning rate secara otomatis ketika metrik validasi tidak membaik.

Fungsi train dipanggil dengan parameter lengkap, termasuk model, DataLoader, fungsi loss, optimizer, scheduler, jumlah epoch, dan device, di mana fungsi ini juga bertanggung jawab menyimpan bobot model terbaik dan terakhir secara otomatis sesuai path yang telah ditentukan.

Setelah pelatihan selesai, hasil metrik seperti loss dan akurasi yang tersimpan dalam variabel history divisualisasikan menggunakan fungsi visualize_history untuk memudahkan analisis performa model selama proses training dan validasi.

Secara keseluruhan, kode ini mengatur alur lengkap mulai dari loading data, persiapan model, pelatihan dengan monitoring performa, hingga visualisasi hasil, khusus untuk kasus klasifikasi video dengan model ConvLSTM, dan struktur modular serta penggunaan parameter terpisah membuatnya mudah dikonfigurasi dan diadaptasi untuk eksperimen lebih lanjut.
"""

# main.py (Updated)
import torch
from torch.utils.data import DataLoader
# from torchvision import datasets, transforms # Not needed for VideoDataset
from model import ClassificationModelConvLSTM  # Import from model.py
from train import train, evaluate # This should now work after creating train.py
from utils import visualize_history
import parameters as params
from google.colab import drive
import os
import pickle
import numpy as np
from glob import glob
from torchvision import transforms
from sklearn.model_selection import StratifiedShuffleSplit

# Mount Google Drive (jika pakai Colab)
# Ensure you run the cell that mounts Google Drive first
# drive.mount('/content/gdrive') # Assuming this has been run in a previous cell

# Define Transform (re-defined here for clarity of this script)
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((params.model_input_size[0], params.model_input_size[1])),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load Dataset (Use your VideoDataset)
# Assuming you have already downloaded and extracted the UCF50 dataset
# and created the real_lab2idx.pickle file in previous cells.
PATH_DATASET_DIR = "/content/UCF50"
videos = glob(f"{PATH_DATASET_DIR}/**/*.avi", recursive=True)

# Load the full dataset first
full_dataset = VideoDataset(videos, transform=transform, num_videos_per_class=100) # Use your VideoDataset

# Get labels for stratified split
all_labels = [full_dataset[i][1] for i in range(len(full_dataset))]

# Stratified split: train (70%), val (20%), test (10%)
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
train_idx, temp_idx = next(splitter.split(np.zeros(len(all_labels)), all_labels))

# Split val/test from temp_idx (val: 20%, test: 10%)
temp_labels = [all_labels[i] for i in temp_idx]
# Calculate the fraction for the second split: test_size = test_fraction / (val_fraction + test_fraction)
# Here, val is 20% and test is 10% of the original dataset, so from the 30% temp_idx,
# val should be 2/3 of temp_idx (20% / 30%) and test should be 1/3 (10% / 30%).
splitter = StratifiedShuffleSplit(n_splits=1, test_size=1/3, random_state=42) # Corrected test_size for 2nd split
val_idx, test_idx = next(splitter.split(np.zeros(len(temp_labels)), temp_labels))

# Mapping temp_idx indices to original dataset indices
val_idx = [temp_idx[i] for i in val_idx]
test_idx = [temp_idx[i] for i in test_idx]

# Create subset dataset
train_dataset = torch.utils.data.Subset(full_dataset, train_idx)
val_dataset = torch.utils.data.Subset(full_dataset, val_idx)
test_dataset = torch.utils.data.Subset(full_dataset, test_idx)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True, drop_last=True, num_workers=params.num_workers)
val_loader = DataLoader(val_dataset, batch_size=params.batch_size, shuffle=False, drop_last=True, num_workers=params.num_workers)
test_loader = DataLoader(test_dataset, batch_size=params.batch_size, shuffle=False, num_workers=params.num_workers)

# Inisialisasi model - Use correct input_dim (channels) and input_size
model = ClassificationModelConvLSTM(
    input_dim=params.model_input_dim, # Number of channels (e.g., 3 for RGB)
    hidden_size=params.hidden_size,
    num_layers=params.num_lstm_layers,
    num_classes=params.num_classes,
    input_size=params.model_input_size # Spatial dimensions (H, W) after transform
).to(params.device)

# ✅ Tambahkan weight_decay di sini
optimizer = torch.optim.Adam(model.parameters(), lr=params.learning_rate, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)

# Train model
# Ensure the paths in parameters.py are correct and Google Drive is mounted
model, history = train(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    loss_fn=params.loss_fn,
    optimizer=optimizer,
    epochs=params.epochs,
    device=params.device,
    save_best_weights_path=params.best_model_path,
    save_last_weights_path=params.last_weight_template,
    scheduler=scheduler # Pass the scheduler
)

# Visualisasi hasil training
visualize_history(history)

"""Kedua grafik menggambarkan performa model ConvLSTM selama 50 epoch, baik pada data pelatihan (train) maupun data validasi (val). Garis merah menunjukkan metrik untuk data pelatihan (train_acc dan train_loss), sementara garis hijau menunjukkan metrik untuk data validasi (val_acc dan val_loss).

Dari grafik akurasi, terlihat bahwa akurasi validasi meningkat tajam pada 10–15 epoch pertama, dan kemudian cenderung stabil di angka sekitar 0.75 setelah epoch ke-20. Akurasi pelatihan juga meningkat seiring waktu, tetapi tetap lebih rendah dari akurasi validasi, stabil di sekitar 0.50–0.52. Ini bisa menjadi indikasi bahwa model belum belajar dengan optimal pada data pelatihan, atau ada regularisasi yang cukup kuat seperti dropout atau augmentasi.

Pada grafik loss, baik train_loss maupun val_loss mengalami penurunan yang cukup tajam di awal pelatihan, terutama hingga epoch ke-10. Setelah itu, penurunan berlangsung lebih lambat dan relatif stabil. val_loss menurun lebih drastis dan mencapai nilai lebih rendah dari train_loss, yang sekali lagi mengindikasikan bahwa model lebih baik dalam memprediksi data validasi dibandingkan data pelatihan.

Hal ini cukup menarik, karena biasanya train_acc lebih tinggi daripada val_acc, dan train_loss lebih rendah daripada val_loss. Fenomena sebaliknya (yakni val_acc > train_acc dan val_loss < train_loss) bisa disebabkan oleh beberapa hal, seperti: (1) data validasi yang lebih “mudah” dipelajari, (2) penggunaan teknik regularisasi yang kuat, atau (3) kemungkinan adanya label noise atau kesalahan anotasi dalam data pelatihan.

Dari sudut pandang pembelajaran mesin, hasil ini bisa dianggap cukup baik karena val_loss terus menurun tanpa tanda-tanda overfitting. Model tidak mengalami degradasi performa pada data validasi, dan bahkan menunjukkan generalisasi yang cukup kuat. Namun, rendahnya train_acc mungkin menjadi perhatian dan perlu dikaji ulang, misalnya dengan melonggarkan regularisasi, menambah kapasitas model, atau memperbaiki kualitas data pelatihan.

Secara keseluruhan, grafik ini menunjukkan bahwa model ConvLSTM telah mencapai konvergensi dengan stabil dan menunjukkan performa validasi yang baik. Untuk perbaikan ke depan, bisa dipertimbangkan untuk melakukan tuning hyperparameter lebih lanjut, menambah epoch jika diperlukan, atau mengevaluasi ulang distribusi label di antara data pelatihan dan validasi agar tidak terjadi mismatch.
"""



"""## Display Training History Table

Kode berikut ini bertujuan untuk menampilkan riwayat proses pelatihan (training history) model machine learning dalam bentuk tabel yang mudah dibaca menggunakan pandas DataFrame. Biasanya, saat melatih model, kita mencatat metrik seperti loss dan accuracy untuk data pelatihan (train) dan validasi (validation) di setiap epoch.

Kode ini mengambil jumlah epoch dari konfigurasi model (params.config), lalu menggunakan data yang telah disimpan di history (yang biasanya diisi selama proses pelatihan), dan membungkusnya dalam sebuah DataFrame. Tabel ini berisi kolom: Epoch, Train Loss, Train Accuracy, Validation Loss, dan Validation Accuracy. Tujuannya agar kita bisa memantau perkembangan performa model dari epoch ke epoch, misalnya untuk melihat apakah model mengalami overfitting, underfitting, atau justru belajar dengan baik.

Akhirnya, tabel ini ditampilkan ke layar menggunakan print() atau, jika dalam Jupyter Notebook, bisa ditampilkan lebih rapi menggunakan display().
"""

# Cell baru di luar main.py
import pandas as pd

# Ambil epochs dari config
epochs = params.epochs

# Buat DataFrame dari history
df = pd.DataFrame({
    "Epoch": list(range(1, epochs + 1)),
    "Train Loss": history['train_loss'],
    "Train Accuracy": history['train_acc'],
    "Validation Loss": history['val_loss'],
    "Validation Accuracy": history['val_acc'],
})

# Tampilkan tabel
print(df)

# (Optional) kalau mau tabel yang lebih cantik pakai display()
from IPython.display import display
display(df)

"""Berikut adalah penjelasan dari tabel dari Hasil Pelatihan Model per Epoch tersebut:
1. Perkembangan awal sangat signifikan
Selama 10 epoch pertama, terlihat penurunan loss yang cukup tajam baik pada data pelatihan maupun validasi. Train loss menurun dari sekitar 3.7 ke 2.08, sementara validation loss dari 3.2 ke 1.26. Bersamaan dengan itu, akurasi validasi meningkat dari 22.2% ke sekitar 68.6%. Ini menunjukkan bahwa model dengan cepat belajar dari data dan mulai memahami pola yang penting.

2. Titik jenuh mulai tercapai
Setelah sekitar epoch ke-20, laju penurunan validation loss mulai melambat, dan akurasi validasi mendatar di sekitar 70%. Ini menunjukkan bahwa model mulai mendekati batas kemampuan generalisasinya terhadap data validasi. Akurasi validasi masih naik perlahan sampai sekitar 75%, tetapi tidak sebesar peningkatan awal.

3. Tidak terjadi overfitting yang parah
Perbandingan antara train loss dan validation loss tidak menunjukkan divergensi besar, dan akurasi pelatihan hanya sedikit lebih rendah daripada akurasi validasi. Hal ini bisa menunjukkan bahwa model tidak mengalami overfitting yang serius, meskipun sedikit fluktuasi minor tetap terjadi, seperti saat train loss menurun tetapi validation loss sedikit naik.

4. Stabilitas model pada akhir pelatihan
Mulai dari epoch ke-30 hingga ke-50, nilai validation loss cenderung stabil sekitar 1.02 hingga 1.05, dan validation accuracy stabil di atas 74%, bahkan mencapai 75.4% pada epoch ke-42. Ini menandakan bahwa model sudah berada dalam fase konvergen dan relatif stabil, sehingga pelatihan dapat dihentikan di titik ini tanpa risiko besar kehilangan performa.

5. Potensi optimasi lebih lanjut
Meskipun performa validasi sudah cukup baik, akurasi pelatihan tidak melebihi 52%, yang menandakan bahwa model masih bisa belajar lebih baik jika diberi lebih banyak kapasitas (misalnya model yang lebih dalam) atau data tambahan. Alternatif lain adalah mencoba strategi regularisasi atau fine-tuning agar generalisasi meningkat tanpa memaksa model belajar noise.

## Evaluating ConvLSTM Model on Test Dataset

Kode ini bertujuan untuk memuat model klasifikasi video berbasis ConvLSTM (Convolutional LSTM), memuat bobot model yang telah dilatih sebelumnya, dan mengevaluasi performanya pada dataset pengujian (test set).

Prosesnya dimulai dengan mengimpor library yang diperlukan serta model ClassificationModelConvLSTM. Kemudian, kode memuat file label2idx, yang berisi pemetaan label ke indeks numerik. Model ClassificationModelConvLSTM diinisialisasi untuk jumlah kelas yang sesuai dan dipindahkan ke perangkat (CPU atau GPU). Fungsi evaluate disediakan untuk melakukan inferensi tanpa proses pelatihan, menghitung loss total dan akurasi pada seluruh dataset uji yang disediakan oleh DataLoader.

Fungsi ini menjalankan model dalam mode evaluasi (model.eval()), menghitung prediksi, mengakumulasi loss dan jumlah prediksi yang benar. Hasil akhirnya berupa average loss dan accuracy, yang dicetak ke layar.

Tujuan utama kode ini adalah untuk mengevaluasi kinerja model setelah proses pelatihan selesai, sehingga kita bisa mengetahui seberapa baik model generalisasi terhadap data yang belum pernah dilihat sebelumnya (test set).
"""

from train import evaluate # Import evaluate from train.py
from model import ClassificationModelConvLSTM as ModelClass # Import ClassificationModelConvLSTM from model.py
import torch
import glob
from torch.utils.data import DataLoader
from dataset import VideoDataset
import parameters as params
from torchvision import transforms # Import transforms for the test dataset

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load model
num_classes = params.num_classes
# Use the correct model class and input_size
model = ModelClass(num_classes=num_classes, input_size=params.model_input_size).to(device)


# Load best weights
# Check if the best model file exists before loading
if os.path.exists(params.best_model_path):
    model.load_state_dict(torch.load(params.best_model_path))
    print(f"Loaded best model weights from {params.best_model_path}")
else:
    print(f"Best model weights not found at {params.best_model_path}. Cannot evaluate.")


model.eval()

# Define Transform for the test dataset (same as training)
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((params.model_input_size[0], params.model_input_size[1])),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])


# Dataset & Loader
# Use the correct path for test data if it's different, otherwise use the main UCF50 path and split
# Assuming test data is within the UCF50 directory or a dedicated 'test' split is needed from the full dataset
# Based on the previous split, the test_dataset is already created and contains the correct subset of videos.
# We can reuse the test_dataset created in the main.py cell or recreate it here.
# Recreating it here for clarity, assuming test videos are in subfolders of UCF50.
# If you used a stratified split in main.py, it's better to reuse the test_dataset object.
# For simplicity here, I'll assume test data can be loaded using VideoDataset directly from UCF50,
# but this might not reflect the exact stratified split used in training.
# A more robust approach would be to save/load the split indices or the subset datasets.

# Since the `test_dataset` object was created and populated in the `main.py` cell and is in the kernel state,
# it's best to reuse that object directly.
# Make sure the `main.py` cell has been executed successfully before running this cell.

if 'test_dataset' in locals():
    print("Reusing test_dataset from previous cell.")
    test_loader = DataLoader(test_dataset, batch_size=params.batch_size, shuffle=False, num_workers=params.num_workers)
    # Check the size of the test_loader to confirm it has data
    print(f"Test DataLoader has {len(test_loader)} batches.")
else:
    print("test_dataset not found. Attempting to load test data directly from UCF50.")
    # This part is a fallback and might not use the exact stratified split
    test_video_paths = glob.glob(f'{params.PATH_DATASET_DIR}/**/*.avi', recursive=True)
    test_dataset = VideoDataset(test_video_paths, transform=transform, sequence_length=params.sequence_length)

    # It's better to use the indices from the stratified split to create the test_dataset
    # if the main.py cell was run and populated test_idx.
    if 'test_idx' in locals():
         print("Using test_idx from stratified split to create test_dataset.")
         test_dataset = torch.utils.data.Subset(full_dataset, test_idx) # Assuming full_dataset and test_idx are available
         test_loader = DataLoader(test_dataset, batch_size=params.batch_size, shuffle=False, num_workers=params.num_workers)
         print(f"Test DataLoader created from stratified split with {len(test_loader)} batches.")
    else:
        # Fallback if no stratified split info is available
        print("Stratified split indices not found. Creating test_loader from all videos with a simple split assumption.")
        # This is a less ideal scenario if stratified split was intended
        # You might need to adjust this part based on how your test data is organized or loaded
        # For this example, let's just create a DataLoader from the potentially full dataset (or a simplified assumption)
        test_loader = DataLoader(test_dataset, batch_size=params.batch_size, shuffle=False, num_workers=params.num_workers)
        print(f"Test DataLoader created as a fallback with {len(test_loader)} batches.")


# Loss function
loss_fn = params.loss_fn

# Evaluate
# Pass verbose=True to see the evaluation progress
test_loss, test_acc = evaluate(model, test_loader, loss_fn, device, verbose=True)
print(f'🎯 Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_acc:.4f}')

"""Hasil evaluasi yang ditampilkan menunjukkan bahwa model telah memuat bobot terbaik yang disimpan selama pelatihan dari direktori /content/gdrive/MyDrive/weights/best_model.pt. Bobot ini biasanya diambil saat model mencapai performa terbaik pada data validasi, seperti ketika nilai val_loss paling rendah atau val_accuracy paling tinggi. Penggunaan bobot terbaik ini penting untuk memastikan bahwa model yang diuji bukan yang terakhir dilatih, tetapi yang benar-benar paling optimal dari proses pelatihan.

Selanjutnya, tercatat bahwa dataset pengujian (test dataset) digunakan ulang dari sel sebelumnya. Ini menunjukkan bahwa data pengujian sudah disiapkan lebih dulu dan kini dimanfaatkan kembali tanpa perlu pemrosesan ulang. Hal ini efisien secara waktu dan memori, terutama dalam lingkungan interaktif seperti Google Colab. Dataset tersebut umumnya telah melalui tahap preprocessing seperti pembersihan teks, tokenisasi, padding, serta konversi ke vektor atau indeks numerik yang dibutuhkan oleh model.

Evaluasi dilakukan menggunakan DataLoader yang membagi dataset pengujian ke dalam 125 batch. Ini berarti keseluruhan data pengujian dibagi ke dalam kelompok kecil untuk memungkinkan evaluasi yang efisien dalam memori. Bila batch_size yang digunakan adalah 32, maka jumlah total sampel pengujian adalah sekitar 4.000. DataLoader ini juga memastikan bahwa proses evaluasi dilakukan secara terstruktur, batch demi batch.

Progress bar evaluasi menunjukkan bahwa semua 125 batch telah berhasil dievaluasi dalam waktu sekitar 12 detik, dengan kecepatan rata-rata 9.78 batch per detik. Progress bar ini berguna untuk memantau jalannya evaluasi, memberikan informasi real-time terkait kecepatan dan estimasi waktu selesai. Hal ini menegaskan bahwa evaluasi berjalan lancar dan efisien, tanpa kendala teknis.

Setelah evaluasi selesai, diperoleh metrik utama yaitu nilai loss dan accuracy pada data pengujian. Nilai final test loss adalah 0.9652 yang menunjukkan seberapa baik model memperkirakan label sebenarnya; semakin kecil nilainya, semakin baik. Sementara itu, akurasi pengujian mencapai 0.7440 atau 74.40%, yang menandakan bahwa 3 dari 4 prediksi model pada data yang belum pernah dilihat sebelumnya sudah benar. Ini adalah hasil yang sangat baik dan konsisten dengan akurasi validasi yang ditunjukkan pada grafik sebelumnya.

Secara keseluruhan, hasil ini menunjukkan bahwa model ConvLSTM yang dilatih tidak mengalami overfitting secara signifikan dan mampu mempertahankan performa yang baik pada data pengujian. Ini mengindikasikan bahwa model memiliki generalisasi yang baik terhadap data baru, yang merupakan tujuan utama dari proses pelatihan model pembelajaran mesin. Keberhasilan memuat bobot terbaik dan konsistensi hasil evaluasi menandakan pipeline pelatihan dan evaluasi telah dirancang dengan baik.
"""